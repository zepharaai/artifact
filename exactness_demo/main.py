
# exactness_demo_v2: Byte-exact deterministic replay under G1 (CPU, NumPy)
# Produces a reviewer-verifiable JSON proof with model and optimizer byte-equality.
# Author: generated by ChatGPT

import os, json, struct, zlib, hashlib, math
import numpy as np
from dataclasses import dataclass

# ------------------------------
# Utilities
# ------------------------------
def blake2b_64(data: bytes) -> bytes:
    return hashlib.blake2b(data, digest_size=8).digest()

def sha256_bytes(xs: bytes) -> str:
    return hashlib.sha256(xs).hexdigest()

def pack_u64_le(x: int) -> bytes:
    return struct.pack('<Q', x)

def pack_u32_le(x: int) -> bytes:
    return struct.pack('<I', x)

def pack_u16_le(x: int) -> bytes:
    return struct.pack('<H', x)

def pack_u8(x: int) -> bytes:
    return struct.pack('<B', x)

def pack_f32_le(x: float) -> bytes:
    return struct.pack('<f', float(x))

def crc32(b: bytes) -> int:
    return zlib.crc32(b) & 0xffffffff

def seed64(base_seed: int, logical_step: int, mb_index: int) -> int:
    # Derive a 64-bit seed deterministically from (base_seed, step, mb_index)
    h = hashlib.blake2b(digest_size=8)
    h.update(struct.pack('<QQQ', base_seed & 0xffffffffffffffff, logical_step & 0xffffffffffffffff, mb_index & 0xffffffffffffffff))
    return int.from_bytes(h.digest(), 'little')

def hash64_ids(ordered_ids):
    # Content hash of ordered sample IDs (64-bit)
    h = hashlib.blake2b(digest_size=8)
    for i in ordered_ids:
        h.update(struct.pack('<Q', int(i)))
    return h.digest()  # 8 bytes

# ------------------------------
# Model: 2-layer MLP (ReLU + dropout) in NumPy
# Loss: 0.5 * sum((y_pred - y)^2)  (sum reduction)
# ------------------------------
@dataclass
class Params:
    W1: np.ndarray  # D x H, float32
    b1: np.ndarray  # H
    W2: np.ndarray  # H x O
    b2: np.ndarray  # O

@dataclass
class OptimState:
    m_W1: np.ndarray
    v_W1: np.ndarray
    m_b1: np.ndarray
    v_b1: np.ndarray
    m_W2: np.ndarray
    v_W2: np.ndarray
    m_b2: np.ndarray
    v_b2: np.ndarray
    step: int  # Adam step counter (increments only when an update is applied)

def init_model(D=8, H=16, O=4, seed=123):
    rng = np.random.RandomState(seed)
    # He init for relu
    W1 = rng.randn(D, H).astype(np.float32) * np.sqrt(2.0/D)
    b1 = np.zeros((H,), dtype=np.float32)
    W2 = rng.randn(H, O).astype(np.float32) * np.sqrt(2.0/H)
    b2 = np.zeros((O,), dtype=np.float32)
    return Params(W1, b1, W2, b2)

def init_optim_like(p: Params):
    zeros = lambda a: np.zeros_like(a, dtype=np.float32)
    return OptimState(
        zeros(p.W1), zeros(p.W1),
        zeros(p.b1), zeros(p.b1),
        zeros(p.W2), zeros(p.W2),
        zeros(p.b2), zeros(p.b2),
        step=0
    )

def relu(x): return np.maximum(x, 0.0, dtype=np.float32)

def forward_and_backward(params: Params, x, y, keep_prob, dropout_seed):
    """
    x: (B, D), y: (B, O), both float32
    dropout_keep_prob in (0,1]; dropout applied to hidden activations
    Uses per-microbatch RNG seed for reproducible mask.
    Returns grads dict with SUM reduction over batch.
    """
    # Forward
    z1 = x.dot(params.W1) + params.b1  # (B,H)
    a1 = relu(z1)
    # Dropout
    rng = np.random.RandomState(dropout_seed & 0xffffffff)  # RandomState takes 32-bit seed; derive from 64 by truncation
    mask = (rng.rand(*a1.shape) < keep_prob).astype(np.float32)
    a1_do = a1 * mask / keep_prob  # inverted dropout
    y_pred = a1_do.dot(params.W2) + params.b2  # (B,O)
    # Loss (sum reduction): 0.5 * sum((y_pred - y)^2)
    diff = (y_pred - y).astype(np.float32)
    # Backward
    dyp = diff  # dL/dy_pred = (y_pred - y), sum reduction
    dW2 = (a1_do.T).dot(dyp)  # (H,B)@(B,O) => (H,O)
    db2 = np.sum(dyp, axis=0)  # (O,)
    da1_do = dyp.dot(params.W2.T)  # (B,O)@(O,H) => (B,H)
    da1 = da1_do * mask / keep_prob  # back through dropout
    dz1 = da1 * (z1 > 0).astype(np.float32)  # relu'
    dW1 = (x.T).dot(dz1)  # (D,B)@(B,H) => (D,H)
    db1 = np.sum(dz1, axis=0)  # (H,)
    grads = {
        'W1': dW1.astype(np.float32), 'b1': db1.astype(np.float32),
        'W2': dW2.astype(np.float32), 'b2': db2.astype(np.float32),
    }
    return grads

def adamw_update(params: Params, opt: OptimState, grads, lr, wd, beta1=0.9, beta2=0.999, eps=1e-8):
    """Decoupled weight decay; update only if grads is not None (i.e., logical step had contributions)."""
    def upd(w, m, v, g):
        m[:] = beta1 * m + (1-beta1) * g
        v[:] = beta2 * v + (1-beta2) * (g * g)
        mhat = m / (1 - (beta1 ** (opt.step)))
        vhat = v / (1 - (beta2 ** (opt.step)))
        # decoupled weight decay
        w[:] = w - lr * wd * w - lr * (mhat / (np.sqrt(vhat) + eps))
    if grads is None:
        return  # skip
    # increment Adam step first (so t matches the step count of applied updates)
    opt.step += 1
    upd(params.W1, opt.m_W1, opt.v_W1, grads['W1'])
    opt.m_b1[:] = 0.9 * opt.m_b1 + 0.1 * grads['b1']
    opt.v_b1[:] = 0.999 * opt.v_b1 + 0.001 * (grads['b1'] * grads['b1'])
    mhat_b1 = opt.m_b1 / (1 - (0.9 ** (opt.step)))
    vhat_b1 = opt.v_b1 / (1 - (0.999 ** (opt.step)))
    params.b1[:] = params.b1 - lr * wd * params.b1 - lr * (mhat_b1 / (np.sqrt(vhat_b1) + 1e-8))

    upd(params.W2, opt.m_W2, opt.v_W2, grads['W2'])
    opt.m_b2[:] = 0.9 * opt.m_b2 + 0.1 * grads['b2']
    opt.v_b2[:] = 0.999 * opt.v_b2 + 0.001 * (grads['b2'] * grads['b2'])
    mhat_b2 = opt.m_b2 / (1 - (0.9 ** (opt.step)))
    vhat_b2 = opt.v_b2 / (1 - (0.999 ** (opt.step)))
    params.b2[:] = params.b2 - lr * wd * params.b2 - lr * (mhat_b2 / (np.sqrt(vhat_b2) + 1e-8))

# ------------------------------
# Data and batching
# ------------------------------
def make_dataset(N=96, D=8, O=4, seed=202):
    rng = np.random.RandomState(seed)
    X = rng.randn(N, D).astype(np.float32)
    # True linear->relu->linear generator
    Wt1 = rng.randn(D, 12).astype(np.float32) * 0.5
    bt1 = rng.randn(12).astype(np.float32) * 0.1
    Wt2 = rng.randn(12, O).astype(np.float32) * 0.5
    bt2 = rng.randn(O).astype(np.float32) * 0.1
    h = np.maximum(0, X.dot(Wt1) + bt1)
    Y = h.dot(Wt2) + bt2 + 0.01 * rng.randn(N, O).astype(np.float32)
    return X, Y

def plan_microbatches(N, mb_size, accum, pattern='retain,forget,retain'):
    """
    Returns list of microbatches (each is ordered list of sample IDs).
    We'll build a pattern:
      first block: retain-only chunk
      second block: forget-only chunk
      third block: retain-only chunk
    so that some logical steps go empty after filtering.
    """
    ids = np.arange(N, dtype=np.int64)
    # Partition ids into three equal-ish chunks
    third = N // 3
    blocks = [ids[:third], ids[third:2*third], ids[2*third:]]
    # microbatches in order:
    # block0 (retain), block1 (forget), block2 (retain)
    order = np.concatenate(blocks, axis=0)
    mbs = []
    for i in range(0, N, mb_size):
        mbs.append(order[i:i+mb_size].tolist())
    return mbs, blocks

# ------------------------------
# WAL record I/O (32B aligned)
# Record layout (payload = 27B, CRC32 = 4B, pad=1B => 32B total):
#  [hash64:8][seed64:8][lr_f32:4][opt_step_u32:4][accum_end_u8:1][mb_len_u16:2][crc32:4][pad:1]
# ------------------------------
def wal_record(hash64, seed64, lr_f32, opt_step_u32, accum_end_u8, mb_len_u16):
    payload = hash64 + pack_u64_le(seed64) + pack_f32_le(lr_f32) + pack_u32_le(opt_step_u32) + pack_u8(accum_end_u8) + pack_u16_le(mb_len_u16)
    c = pack_u32_le(crc32(payload))
    pad = b'\x00'  # 1 byte to make 32B
    return payload + c + pad

# ------------------------------
# Training passes
# ------------------------------
from dataclasses import dataclass
@dataclass
class WALArtifacts:
    wal_bytes: bytes
    manifest: dict  # maps hash64_hex -> ordered ID list
    per_step_lr: list  # lr value for each logical step
    k_checkpoint: tuple  # (params_ck, optim_ck) after k retain-only steps
    wal_stats: dict     # records_total, records_empty (0 at writer), segment_sha256

def serialize_params(p: Params) -> bytes:
    parts = [p.W1.tobytes(), p.b1.tobytes(), p.W2.tobytes(), p.b2.tobytes()]
    return b''.join(parts)

def serialize_optim(o: OptimState) -> bytes:
    parts = [o.m_W1.tobytes(), o.v_W1.tobytes(),
             o.m_b1.tobytes(), o.v_b1.tobytes(),
             o.m_W2.tobytes(), o.v_W2.tobytes(),
             o.m_b2.tobytes(), o.v_b2.tobytes()]
    parts.append(struct.pack('<Q', o.step))
    return b''.join(parts)

def clone_params(p: Params) -> Params:
    return Params(p.W1.copy(), p.b1.copy(), p.W2.copy(), p.b2.copy())

def clone_optim(o: OptimState) -> OptimState:
    return OptimState(o.m_W1.copy(), o.v_W1.copy(),
                      o.m_b1.copy(), o.v_b1.copy(),
                      o.m_W2.copy(), o.v_W2.copy(),
                      o.m_b2.copy(), o.v_b2.copy(),
                      step=int(o.step))

def train_full_with_WAL(X, Y, mb_plan, accum, k, base_seed, lr0, wd, keep_prob):
    """
    Single full run that writes a WAL across all logical steps.
    We guarantee that the first k logical steps are retain-only by making the forget set fall after.
    """
    # Setup model/optim
    params = init_model(D=X.shape[1], H=16, O=Y.shape[1], seed=7)
    opt = init_optim_like(params)

    wal_bytes = bytearray()
    manifest = {}
    per_step_lr = []  # constant lr here; still record exact value

    records_total = 0

    # Build forget set: middle block (block1) ids are forget
    N = X.shape[0]

    # iterate logical steps
    num_mbs = len(mb_plan)
    g_accum = None
    mb_idx = 0
    applied_steps = 0
    k_ck_saved = False
    first_logical_step = None
    last_logical_step = None

    # Determine total steps T from plan length and accum
    T = math.ceil(num_mbs / accum)

    for t in range(T):
        had_contrib = False
        lr = lr0  # constant for simplicity; could be schedule(t)
        per_step_lr.append(lr)

        for i in range(accum):
            if mb_idx >= num_mbs: 
                continue

            ordered_ids = mb_plan[mb_idx]
            h64 = hash64_ids(ordered_ids)  # 8 bytes
            manifest[h64.hex()] = ordered_ids

            sd64 = seed64(base_seed, t, i)
            mb_len = len(ordered_ids)
            accum_end = 1 if (i == (accum - 1)) else 0
            rec = h64 + struct.pack('<Q', sd64) + struct.pack('<f', lr) + struct.pack('<I', t) + struct.pack('<B', accum_end) + struct.pack('<H', mb_len)
            payload = rec
            c = struct.pack('<I', zlib.crc32(payload) & 0xffffffff)
            wal_rec = payload + c + b'\x00'
            wal_bytes.extend(wal_rec)
            records_total += 1

            # Compute grads on full microbatch
            x_mb = X[ordered_ids]
            y_mb = Y[ordered_ids]
            grads = forward_and_backward(params, x_mb, y_mb, keep_prob, sd64)
            if g_accum is None:
                g_accum = {k:g.copy() for k,g in grads.items()}
            else:
                for k2 in g_accum.keys():
                    g_accum[k2] += grads[k2]
            had_contrib = True
            mb_idx += 1

            if accum_end == 1:
                if had_contrib:
                    if first_logical_step is None: first_logical_step = t
                    last_logical_step = t
                    adamw_update(params, opt, g_accum, lr, wd)
                    applied_steps += 1
                g_accum = None
                had_contrib = False

        if (t == k-1) and not k_ck_saved:
            params_ck = clone_params(params)
            opt_ck = clone_optim(opt)
            k_ck_saved = True

    wal_stats = {
        "records_total": records_total,
        "records_empty": 0,
        "segment_sha256": sha256_bytes(bytes(wal_bytes))
    }
    return WALArtifacts(bytes(wal_bytes), manifest, per_step_lr, (params_ck, opt_ck), wal_stats)

def train_oracle_retain_only(X, Y, mb_plan, accum, base_seed, per_step_lr, wd, keep_prob):
    """Train from theta0 on retain-only (middle third removed), using the filtered plan (skip empty steps)."""
    params = init_model(D=X.shape[1], H=16, O=Y.shape[1], seed=7)
    opt = init_optim_like(params)

    N = X.shape[0]
    third = N // 3
    forget_ids = set(range(third, 2*third))

    num_mbs = len(mb_plan)
    T = math.ceil(num_mbs / accum)

    mb_idx = 0
    applied_steps = 0
    empty_steps = 0
    first_logical_step = None
    last_logical_step = None

    for t in range(T):
        lr = float(per_step_lr[t])
        had_contrib = False
        g_accum = None

        for i in range(accum):
            if mb_idx >= num_mbs: continue
            ordered_ids = mb_plan[mb_idx]
            # Filter forget
            filtered_ids = [sid for sid in ordered_ids if sid not in forget_ids]
            if len(filtered_ids) > 0:
                sd64 = seed64(base_seed, t, i)
                x_mb = X[filtered_ids]
                y_mb = Y[filtered_ids]
                grads = forward_and_backward(params, x_mb, y_mb, keep_prob, sd64)
                if g_accum is None:
                    g_accum = {k:g.copy() for k,g in grads.items()}
                else:
                    for k2 in g_accum.keys():
                        g_accum[k2] += grads[k2]
                had_contrib = True
            mb_idx += 1

            if (i == accum - 1):
                if had_contrib:
                    if first_logical_step is None: first_logical_step = t
                    last_logical_step = t
                    adamw_update(params, opt, g_accum, lr, wd)
                    applied_steps += 1
                else:
                    empty_steps += 1

        # end logical step

    invariants = {
        "applied_steps": applied_steps,
        "empty_logical_steps": empty_steps,
        "first_logical_step": first_logical_step if first_logical_step is not None else -1,
        "last_logical_step": last_logical_step if last_logical_step is not None else -1,
    }
    return params, opt, invariants

def replay_from_Ck(X, Y, wal_bytes, manifest, accum, base_seed, per_step_lr, wd, keep_prob, Ck, k):
    """Deterministic replay from Ck while filtering forget set. Reads WAL records and honors skip-empty steps."""
    params = clone_params(Ck[0])
    opt = clone_optim(Ck[1])

    N = X.shape[0]
    third = N // 3
    forget_ids = set(range(third, 2*third))

    # parse WAL into list of records (32B each)
    rec_size = 32
    assert len(wal_bytes) % rec_size == 0
    recs = [wal_bytes[i:i+rec_size] for i in range(0, len(wal_bytes), rec_size)]

    # We'll iterate records in the same order, but we only apply updates for steps >= k
    applied_steps = 0
    empty_steps = 0
    first_logical_step = None
    last_logical_step = None

    # We'll track by logical step: group by opt_step_u32 and accum_end flag
    g_accum = None
    current_step = None
    had_contrib = False

    for idx, rec in enumerate(recs):
        payload = rec[:27]
        crc = struct.unpack('<I', rec[27:31])[0]
        pad = rec[31]
        assert (zlib.crc32(payload) & 0xffffffff) == crc

        h64 = payload[0:8].hex()
        sd64 = struct.unpack('<Q', payload[8:16])[0]
        lr = struct.unpack('<f', payload[16:20])[0]
        step_u32 = struct.unpack('<I', payload[20:24])[0]
        accum_end = struct.unpack('<B', payload[24:25])[0]
        mb_len = struct.unpack('<H', payload[25:27])[0]

        ordered_ids = manifest[h64]
        filtered_ids = [sid for sid in ordered_ids if sid not in forget_ids]

        if current_step is None:
            current_step = step_u32

        if step_u32 != current_step:
            current_step = step_u32

        if step_u32 < k:
            if accum_end == 1:
                had_contrib = False
                g_accum = None
            continue

        if len(filtered_ids) > 0:
            x_mb = X[filtered_ids]
            y_mb = Y[filtered_ids]
            grads = forward_and_backward(params, x_mb, y_mb, keep_prob, sd64)
            if g_accum is None:
                g_accum = {k:g.copy() for k,g in grads.items()}
            else:
                for k2 in g_accum.keys():
                    g_accum[k2] += grads[k2]
            had_contrib = True

        if accum_end == 1:
            if had_contrib:
                if first_logical_step is None: first_logical_step = step_u32
                last_logical_step = step_u32
                lr_effective = float(per_step_lr[step_u32])
                adamw_update(params, opt, g_accum, lr_effective, wd)
                applied_steps += 1
            else:
                empty_steps += 1
            g_accum = None
            had_contrib = False

    invariants = {
        "applied_steps": applied_steps,
        "empty_logical_steps": empty_steps,
        "first_logical_step": first_logical_step if first_logical_step is not None else -1,
        "last_logical_step": last_logical_step if last_logical_step is not None else -1,
    }
    return params, opt, invariants

def compare_optim_states(o1: OptimState, o2: OptimState):
    eq = {}
    eq['exp_avg_equal'] = (np.array_equal(o1.m_W1, o2.m_W1) and
                           np.array_equal(o1.m_b1, o2.m_b1) and
                           np.array_equal(o1.m_W2, o2.m_W2) and
                           np.array_equal(o1.m_b2, o2.m_b2))
    eq['exp_avg_sq_equal'] = (np.array_equal(o1.v_W1, o2.v_W1) and
                              np.array_equal(o1.v_b1, o2.v_b1) and
                              np.array_equal(o1.v_W2, o2.v_W2) and
                              np.array_equal(o1.v_b2, o2.v_b2))
    eq['step_equal'] = (o1.step == o2.step)
    mismatched = []
    if not eq['exp_avg_equal']: mismatched.append('exp_avg')
    if not eq['exp_avg_sq_equal']: mismatched.append('exp_avg_sq')
    if not eq['step_equal']: mismatched.append('step')
    eq['mismatched'] = mismatched
    return eq

def sha_params(p): return hashlib.sha256(serialize_params(p)).hexdigest()
def sha_optim(o): return hashlib.sha256(serialize_optim(o)).hexdigest()

def main():
    # Config
    D, O = 8, 4
    N = 96
    MB_SIZE = 8
    ACCUM = 2
    BASE_SEED = 1337
    KEEP_PROB = 0.8
    LR0 = 1e-3
    WD = 0.01
    K = 2  # checkpoint after K retain-only steps

    # Data
    X, Y = make_dataset(N=N, D=D, O=O, seed=202)
    mb_plan, blocks = plan_microbatches(N, MB_SIZE, ACCUM)

    # Writer pass with WAL, and capture Ck after K steps
    arts = train_full_with_WAL(X, Y, mb_plan, ACCUM, K, BASE_SEED, LR0, WD, KEEP_PROB)
    wal_bytes = arts.wal_bytes
    manifest = arts.manifest
    per_step_lr = arts.per_step_lr
    (Ck_params, Ck_opt) = arts.k_checkpoint
    wal_stats = arts.wal_stats

    # Oracle retain-only training from theta0
    oracle_params, oracle_opt, inv_oracle = train_oracle_retain_only(
        X, Y, mb_plan, ACCUM, BASE_SEED, per_step_lr, WD, KEEP_PROB
    )

    # Replay from Ck with filtering
    replay_params, replay_opt, inv_replay = replay_from_Ck(
        X, Y, wal_bytes, manifest, ACCUM, BASE_SEED, per_step_lr, WD, KEEP_PROB, (Ck_params, Ck_opt), K
    )

    # Digests
    Ck_model_sha = sha_params(Ck_params)
    Ck_opt_sha   = sha_optim(Ck_opt)
    Oracle_model_sha = sha_params(oracle_params)
    Oracle_opt_sha   = sha_optim(oracle_opt)
    Replay_model_sha = sha_params(replay_params)
    Replay_opt_sha   = sha_optim(replay_opt)

    # Optim component equality
    opt_cmp = compare_optim_states(oracle_opt, replay_opt)

    status = "PASS" if (Oracle_model_sha == Replay_model_sha and Oracle_opt_sha == Replay_opt_sha and opt_cmp['exp_avg_equal'] and opt_cmp['exp_avg_sq_equal'] and opt_cmp['step_equal']) else "FINAL_MISMATCH"

    proof = {
        "status": status,
        "Ck_model": Ck_model_sha,
        "Ck_optim": Ck_opt_sha,
        "Oracle_model": Oracle_model_sha,
        "Oracle_optim": Oracle_opt_sha,
        "Replay_model": Replay_model_sha,
        "Replay_optim": Replay_opt_sha,
        "optimizer_component_equality": opt_cmp,
        "replay_invariants": inv_replay,
        "oracle_invariants": inv_oracle,
        "wal_stats": wal_stats,
        "config": { "N": N, "D": D, "O": O, "mb_size": MB_SIZE, "accum": ACCUM, "K": K, "base_seed": BASE_SEED, "keep_prob": KEEP_PROB, "lr0": LR0, "wd": WD },
        "notes": "CPU-only NumPy demo; loss=sum; per-microbatch seeds; per-step lr stored; skip-empty logical steps; hash64 is blake2b(ordered IDs)."
    }

    # Persist WAL and JSON proof
    out_wal = "/mnt/data/run.wal"
    with open(out_wal, "wb") as f:
        f.write(wal_bytes)
    out_json = "/mnt/data/equality_proof_v2.json"
    with open(out_json, "w") as f:
        json.dump(proof, f, indent=2)

    # Also print a concise summary
    print("[STATUS]", status)
    print("Ck model:", Ck_model_sha)
    print("Ck optim:", Ck_opt_sha)
    print("Oracle model:", Oracle_model_sha)
    print("Replay model:", Replay_model_sha)
    print("Oracle optim:", Oracle_opt_sha)
    print("Replay optim:", Replay_opt_sha)
    print("Optimizer components equal:", proof["optimizer_component_equality"])
    print("Replay invariants:", inv_replay)
    print("Oracle invariants:", inv_oracle)
    print("WAL:", out_wal, "sha256:", wal_stats["segment_sha256"])
    print("JSON proof:", out_json)

if __name__ == "__main__":
    main()
